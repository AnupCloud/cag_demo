{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ww0BKY1HEfVY",
      "metadata": {
        "id": "ww0BKY1HEfVY"
      },
      "source": [
        "# Cache-Augmented Generation\n",
        "\n",
        "This notebook is a demonstration of **Cache-Augmented Generation** using:\n",
        "- **Mistral** (`mistralai/Mistral-7B-Instruct-v0.1`)\n",
        "- A `document.txt` file describing **YOU**.\n",
        "- A simple **KV cache** mechanism with `DynamicCache`.\n",
        "\n",
        "We’ll:\n",
        "1. Load the model.\n",
        "2. Preload `document.txt` into the cache.\n",
        "3. Ask two questions, reusing the same cache.\n",
        "\n",
        "Prerequisites:\n",
        "1. A HuggingFace account\n",
        "2. A .env file with your HuggingFace access token.\n",
        "3. A document.txt file with sentences about yourself.\n",
        "\n",
        "For this demo, I will be using my own document.txt and ask questions about myself (Ronan Takizawa)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mcHsquxWGLdB",
      "metadata": {
        "id": "mcHsquxWGLdB"
      },
      "source": [
        "### Imports and the Generate Function\n",
        "We import the essential libraries (torch, transformers) and define the generate function. This function handles token-by-token generation, reusing the model’s past_key_values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1IqOLKJvEfVa",
      "metadata": {
        "id": "1IqOLKJvEfVa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.cache_utils import DynamicCache\n",
        "import os\n",
        "\n",
        "# Minimal generate function for token-by-token generation\n",
        "def generate(model, input_ids: torch.Tensor, past_key_values, max_new_tokens: int = 50) -> torch.Tensor:\n",
        "    device = model.model.embed_tokens.weight.device\n",
        "    origin_len = input_ids.shape[-1]\n",
        "    input_ids = input_ids.to(device)\n",
        "    output_ids = input_ids.clone()\n",
        "    next_token = input_ids\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            out = model(\n",
        "                input_ids=next_token,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True\n",
        "            )\n",
        "            logits = out.logits[:, -1, :]\n",
        "            token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "            output_ids = torch.cat([output_ids, token], dim=-1)\n",
        "            past_key_values = out.past_key_values\n",
        "            next_token = token.to(device)\n",
        "\n",
        "            if model.config.eos_token_id is not None and token.item() == model.config.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Return just the newly generated part\n",
        "    return output_ids[:, origin_len:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZBD6NNltGP9O",
      "metadata": {
        "id": "ZBD6NNltGP9O"
      },
      "source": [
        "### DynamicCache Setup\n",
        "Initializing the DynamicCache mechanism for storing and reusing the model’s key/value states. It also provides a clean_up function to truncate any extra tokens appended by user queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "UpXP3xKpEfVb",
      "metadata": {
        "id": "UpXP3xKpEfVb"
      },
      "outputs": [],
      "source": [
        "torch.serialization.add_safe_globals([DynamicCache])\n",
        "torch.serialization.add_safe_globals([set])\n",
        "\n",
        "def get_kv_cache(model, tokenizer, prompt: str) -> DynamicCache:\n",
        "    # Encode prompt\n",
        "    device = model.model.embed_tokens.weight.device\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    cache = DynamicCache()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(\n",
        "            input_ids=input_ids,\n",
        "            past_key_values=cache,\n",
        "            use_cache=True\n",
        "        )\n",
        "    return cache\n",
        "\n",
        "def clean_up(cache: DynamicCache, origin_len: int):\n",
        "    # Remove any tokens appended to the original knowledge\n",
        "    for i in range(len(cache.key_cache)):\n",
        "        cache.key_cache[i] = cache.key_cache[i][:, :, :origin_len, :]\n",
        "        cache.value_cache[i] = cache.value_cache[i][:, :, :origin_len, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I7ZH0RwyGmVa",
      "metadata": {
        "id": "I7ZH0RwyGmVa"
      },
      "source": [
        "### .env function logic\n",
        "\n",
        "Defining logic to get Hugging Face Token to log inot Hugging Face and download the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ylo_gh42Fm2p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylo_gh42Fm2p",
        "outputId": "58fb0e16-93b4-45fc-fcd9-d6e4f251ba9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment and imports are set.\n"
          ]
        }
      ],
      "source": [
        "def get_env():\n",
        "    env_dict = {}\n",
        "    env_file = \".env\" if os.path.exists(\".env\") else \"env\"\n",
        "    if os.path.exists(env_file):\n",
        "        with open(env_file, mode=\"r\") as f:\n",
        "            for line in f:\n",
        "                key, value = line.strip().split(\"=\")\n",
        "                env_dict[key] = value.strip('\"')\n",
        "    else:\n",
        "        print(\"No .env or env file found; HF_TOKEN may not be set.\")\n",
        "    return env_dict\n",
        "\n",
        "# env = get_env()\n",
        "# HF_TOKEN = env.get(\"HF_TOKEN\", None)\n",
        "HF_TOKEN = \"API_TOKEN\"\n",
        "\n",
        "# Global placeholders (if needed)\n",
        "model_name = None\n",
        "model = None\n",
        "tokenizer = None\n",
        "rand_seed = None\n",
        "\n",
        "print(\"Environment and imports are set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5diLXLveEfVc",
      "metadata": {
        "id": "5diLXLveEfVc"
      },
      "source": [
        "### Load Mistral\n",
        "We’ll load the `mistralai/Mistral-7B-Instruct-v0.1` model in full precision (FP16 on GPU if available).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f3FdvEEMEfVc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "42865853a09b48f1915e87208177dd8f",
            "2c23301fd4d5403e98bf0789a1726f85",
            "2eeb467349da4bb28ba8d502d1818d50",
            "60f3e84ca67f4e0387e1fe54ffe866b0",
            "643ed63038ee466a8e9d1e2261c2c5b7",
            "45d81d1e167d40b49d3ec0dc0c5c52b1",
            "aa6bd7c250fd421395cfce2fcc08278f",
            "47031a64d0d84af99f558bf9afdb6ae8",
            "15c09430c9124b2a85b7a6d2819434f0",
            "c22907ebfb1444ddb42b115b57c0ac13",
            "a46bdbb0ccf6411fa082e55a71202982"
          ]
        },
        "id": "f3FdvEEMEfVc",
        "outputId": "0e323fa1-9b24-4028-eb70-b4c05ab0b437"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42865853a09b48f1915e87208177dd8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded mistralai/Mistral-7B-Instruct-v0.1.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model.to(device)\n",
        "print(f\"Loaded {model_name}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cV6N8TdEEfVc",
      "metadata": {
        "id": "cV6N8TdEEfVc"
      },
      "source": [
        "### Create a Knowledge Prompt from `document.txt`\n",
        "We read the file, build a short system/user prompt, and call `get_kv_cache`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bfsUydzrEfVc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfsUydzrEfVc",
        "outputId": "9013b972-4f76-420c-da17-17b55cb48e93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KV cache built.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(\"document.txt\"):\n",
        "    raise FileNotFoundError(\"Please create a `document.txt` with info about Ronan Takizawa.\")\n",
        "\n",
        "with open(\"document.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    doc_text = f.read()\n",
        "\n",
        "system_prompt = f\"\"\"\n",
        "<|system|>\n",
        "You are an assistant who provides concise factual answers.\n",
        "<|user|>\n",
        "Context:\n",
        "{doc_text}\n",
        "Question:\n",
        "\"\"\".strip()\n",
        "\n",
        "# Build the cache\n",
        "ronan_cache = get_kv_cache(model, tokenizer, system_prompt)\n",
        "origin_len = ronan_cache.key_cache[0].shape[-2]\n",
        "print(\"KV cache built.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VQjC-WjSEfVc",
      "metadata": {
        "id": "VQjC-WjSEfVc"
      },
      "source": [
        "### Ask Questions Reusing the Cache\n",
        "We use the same knowledge (no real-time retrieval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JZos3PhoEfVc",
      "metadata": {
        "id": "JZos3PhoEfVc"
      },
      "outputs": [],
      "source": [
        "# 1st query\n",
        "question1 = \"Who is Priya Nair?\"\n",
        "clean_up(ronan_cache, origin_len)\n",
        "input_ids_q1 = tokenizer(question1 + \"\\n\", return_tensors=\"pt\").input_ids.to(device)\n",
        "gen_ids_q1 = generate(model, input_ids_q1, ronan_cache)\n",
        "answer1 = tokenizer.decode(gen_ids_q1[0], skip_special_tokens=True)\n",
        "print(\"Q1:\", question1)\n",
        "print(\"A1:\", answer1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7lzogfXzEfVd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lzogfXzEfVd",
        "outputId": "4f188d24-3d09-477f-ffbf-75fd40aa8f0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q2: What are his main projects?\n",
            "A2: Answer: Ronan Takizawa's main projects include TeleSpeech, a text-to-speech Chrome extension that won first place at HackHarvard 2023, and his work as an open-source developer\n"
          ]
        }
      ],
      "source": [
        "# 2nd query\n",
        "question2 = \"What are his main projects?\"\n",
        "clean_up(ronan_cache, origin_len)\n",
        "input_ids_q2 = tokenizer(question2 + \"\\n\", return_tensors=\"pt\").input_ids.to(device)\n",
        "gen_ids_q2 = generate(model, input_ids_q2, ronan_cache)\n",
        "answer2 = tokenizer.decode(gen_ids_q2[0], skip_special_tokens=True)\n",
        "print(\"Q2:\", question2)\n",
        "print(\"A2:\", answer2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b2Sqs2AEfVd",
      "metadata": {
        "id": "7b2Sqs2AEfVd"
      },
      "source": [
        "### Done!\n",
        "This minimal notebook **preloads** knowledge, then answers queries using the **same** cached context."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15c09430c9124b2a85b7a6d2819434f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c23301fd4d5403e98bf0789a1726f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45d81d1e167d40b49d3ec0dc0c5c52b1",
            "placeholder": "​",
            "style": "IPY_MODEL_aa6bd7c250fd421395cfce2fcc08278f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2eeb467349da4bb28ba8d502d1818d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47031a64d0d84af99f558bf9afdb6ae8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15c09430c9124b2a85b7a6d2819434f0",
            "value": 2
          }
        },
        "42865853a09b48f1915e87208177dd8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c23301fd4d5403e98bf0789a1726f85",
              "IPY_MODEL_2eeb467349da4bb28ba8d502d1818d50",
              "IPY_MODEL_60f3e84ca67f4e0387e1fe54ffe866b0"
            ],
            "layout": "IPY_MODEL_643ed63038ee466a8e9d1e2261c2c5b7"
          }
        },
        "45d81d1e167d40b49d3ec0dc0c5c52b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47031a64d0d84af99f558bf9afdb6ae8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60f3e84ca67f4e0387e1fe54ffe866b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c22907ebfb1444ddb42b115b57c0ac13",
            "placeholder": "​",
            "style": "IPY_MODEL_a46bdbb0ccf6411fa082e55a71202982",
            "value": " 2/2 [01:03&lt;00:00, 29.68s/it]"
          }
        },
        "643ed63038ee466a8e9d1e2261c2c5b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a46bdbb0ccf6411fa082e55a71202982": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa6bd7c250fd421395cfce2fcc08278f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c22907ebfb1444ddb42b115b57c0ac13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
